{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from preamble import *\n",
    "plt.rcParams['image.cmap'] = \"gray\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Preprocessing and Scaling\n",
    "#### 3.3.1 Different Kinds of Preprocessing\n",
    "- 스케일링 (Scaling): |척도 ~측정기준???|\n",
    "  - 각 데이터 특성들에 대해 다음과 같은 선형 변환을 통해 전체 특성들의 통계적 분포(스케일)를 동일하게 맞추는 과정. \n",
    "      - | 비교하기 어렵게 단위가 다른 데이터가 들어와도 스케일이 균등해짐|\n",
    "  - 각 특성(Column단위)들의 통계치를 이용한 변환 수행\n",
    "  - 종류\n",
    "    - StandardScaler\n",
    "$$\\dfrac{x_i – mean(x)}{stdev(x)}$$\n",
    "      - 각 특성의 평균을 0, 분산을 1로 변경\n",
    "      - 각 특성의 최소값 및 최대값을 제한하지 않음\n",
    "           - |다른 데이터와 큰 차이를 내는 데이터는 분포가 동떨어진다 <== 단점내용|\n",
    "      - |모든 데이터의 중심 (0.0)|\n",
    "      - 단점: Outlier (이상치)에 영향을 많이 받음<br/><br/>\n",
    "    - RobustScaler\n",
    "$$\\dfrac{x_i – Q_2(x)}{Q_3(x) – Q_1(x)}$$ |4분위값을 이용함, Q1 = 전체의 1/4위치, Q3 = 전체의 3/4위치|\n",
    "      - 중앙값(median |=정렬 후 중앙값|, $Q_2(x)$)과 IQR(Inter-Quartile Range, $Q_3(x) – Q_1(x)$)을 이용해서 척도를 표준화하는 방법\n",
    "      - median 값이 0이 되도록 함\n",
    "      - 전체 특성 값과 매우 동떨어진 특성 값에 영향을 받지 않음 |내용이 이상한데 이상치에 영향 받을거 같음 찾아봐야함|<br/><br/>\n",
    "    - MinMaxScaler\n",
    "$$\\dfrac{x_i – min(x)}{max(x) – min(x)}$$ *이상치 억제 가능\n",
    "      - 모든 특성이 정확하게 0과 1사이에 위치하도록 데이터를 변경\n",
    "      - 2차원 데이터인 경우 모든 데이터가 x축의 0과 1사이, y축의 0과 1사이 사각 영역에 위치하게 됨\n",
    "\n",
    "  - 참고\n",
    "    - http://benalexkeen.com/feature-scaling-with-scikit-learn/\n",
    "    - http://rfriend.tistory.com/269\n",
    "    \n",
    "- 정규화 (Nomalizer)\n",
    "  - 각 특성 벡터의 유클리디안 길이가 1이 되도록 데이터 포인트를 조정\n",
    "  - 즉, 2차원 데이터인 경우 지름이 1인 원에 데이터 포인트를 투영하고 3차원 데이터인 경우 지름이 1인 구에 데이터 포인트를 투영\n",
    "  - 각 데이터(Row단위)들의 통계치를 이용한 변환 수행\n",
    "  - 각 데이터가 서로 다른 비율로 스케일이 조정됨\n",
    "  - 특성 벡터의 길이는 무관하고 데이터의 방향(각도)만이 중요할때 많이 사용\n",
    "  - 종류\n",
    "    - l1\n",
    "    - l2 (기본)\n",
    "    - max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mglearn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-31daa737ad3e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmglearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplots\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot_scaling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'mglearn' is not defined"
     ]
    }
   ],
   "source": [
    "mglearn.plots.plot_scaling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(426, 30)\n",
      "(426,)\n",
      "\n",
      "(143, 30)\n",
      "(143,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=1)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print()\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler(copy=True, feature_range=(0, 1))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지도학습때는 훈련, 정답 다 입력했는데 비지도학습이기 때문에 훈련데이터만 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformed shape: (426, 30)\n",
      "per-feature minimum before scaling:\n",
      " [6.981e+00 9.710e+00 4.379e+01 1.435e+02 5.263e-02 1.938e-02 0.000e+00\n",
      " 0.000e+00 1.060e-01 5.024e-02 1.153e-01 3.602e-01 7.570e-01 6.802e+00\n",
      " 1.713e-03 2.252e-03 0.000e+00 0.000e+00 9.539e-03 8.948e-04 7.930e+00\n",
      " 1.202e+01 5.041e+01 1.852e+02 7.117e-02 2.729e-02 0.000e+00 0.000e+00\n",
      " 1.566e-01 5.521e-02]\n",
      "per-feature maximum before scaling:\n",
      " [2.811e+01 3.928e+01 1.885e+02 2.501e+03 1.634e-01 2.867e-01 4.268e-01\n",
      " 2.012e-01 3.040e-01 9.575e-02 2.873e+00 4.885e+00 2.198e+01 5.422e+02\n",
      " 3.113e-02 1.354e-01 3.960e-01 5.279e-02 6.146e-02 2.984e-02 3.604e+01\n",
      " 4.954e+01 2.512e+02 4.254e+03 2.226e-01 9.379e-01 1.170e+00 2.910e-01\n",
      " 5.774e-01 1.486e-01]\n",
      "per-feature minimum after scaling:\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0.]\n",
      "per-feature maximum after scaling:\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# transform data\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "\n",
    "# print dataset properties before and after scaling\n",
    "print(\"transformed shape: {}\".format(X_train_scaled.shape))\n",
    "\n",
    "# axis=0 --> 426개의 데이터들에 대해 동일한 Colume에 속한 각 특성값들에 대해 MinMaxScaling을 수행함\n",
    "print(\"per-feature minimum before scaling:\\n {}\".format(X_train.min(axis=0))) \n",
    "print(\"per-feature maximum before scaling:\\n {}\".format(X_train.max(axis=0)))\n",
    "print(\"per-feature minimum after scaling:\\n {}\".format(X_train_scaled.min(axis=0)))\n",
    "print(\"per-feature maximum after scaling:\\n {}\".format(X_train_scaled.max(axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "30개 각각 min max값이 나오고 scale 된거는 0,1로 나옴 ||정답 데이터는 scale할 필요 없음 어차피 0 또는 1로 구성됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "per-feature minimum after scaling:\n",
      "[ 0.0336031   0.0226581   0.03144219  0.01141039  0.14128374  0.04406704\n",
      "  0.          0.          0.1540404  -0.00615249 -0.00137796  0.00594501\n",
      "  0.00430665  0.00079567  0.03919502  0.0112206   0.          0.\n",
      " -0.03191387  0.00664013  0.02660975  0.05810235  0.02031974  0.00943767\n",
      "  0.1094235   0.02637792  0.          0.         -0.00023764 -0.00182032]\n",
      "per-feature maximum after scaling:\n",
      "[0.9578778  0.81501522 0.95577362 0.89353128 0.81132075 1.21958701\n",
      " 0.87956888 0.9333996  0.93232323 1.0371347  0.42669616 0.49765736\n",
      " 0.44117231 0.28371044 0.48703131 0.73863671 0.76717172 0.62928585\n",
      " 1.33685792 0.39057253 0.89612238 0.79317697 0.84859804 0.74488793\n",
      " 0.9154725  1.13188961 1.07008547 0.92371134 1.20532319 1.63068851]\n"
     ]
    }
   ],
   "source": [
    "# transform test data\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# print test data properties after scaling\n",
    "print(\"per-feature minimum after scaling:\\n{}\".format(X_test_scaled.min(axis=0)))\n",
    "print(\"per-feature maximum after scaling:\\n{}\".format(X_test_scaled.max(axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 테스트 데이터도 변환해줌\n",
    "  - [주의] 테스트 데이터 변환시에 항상 훈련 데이터들의 통계치만을 사용 |테스트데이터는 단일 데이터일 가능성이 있기 때문에 fit을 할 수 없다. 훈련데이터의 min,max를 가지고 스케일링 함 -> 1이 넘는 데이터가 생성될 수 있다.|\n",
    "  - 즉, 테스트 데이터에 대해 다음과 같은 공식을 통하여 변환\n",
    "$$\\dfrac{x_{test_{ij}} – x_{train_{min_j}}}{x_{train_{max_j}} – x_{train_{min_j}}}$$\n",
    "  - 위 식에서 i는 각 데이터 인덱스, j는 각 데이터들에 대한 특성 인덱스. 즉, min_j와 max_j는 동일한 j번째 특성들 전체에 대한 최소 및 최대값을 의미 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-9914ccb25bcd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# plot the training and test sets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m13\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmglearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcm2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Training set\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'^'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmglearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcm2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Test set\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "# make synthetic data\n",
    "X, _ = make_blobs(n_samples=50, centers=5, random_state=4, cluster_std=2) #1._ = 데이터를 입력하지 않겠다. 2.cluster_std 줄이면 좀 더 뭉쳐짐\n",
    "# split it into training and test sets\n",
    "X_train, X_test = train_test_split(X, random_state=5, test_size=.1)\n",
    "\n",
    "# plot the training and test sets\n",
    "fig, axes = plt.subplots(1, 3, figsize=(13, 4))\n",
    "axes[0].scatter(X_train[:, 0], X_train[:, 1], c=mglearn.cm2(0), label=\"Training set\", s=60)\n",
    "axes[0].scatter(X_test[:, 0], X_test[:, 1], marker='^', c=mglearn.cm2(1), label=\"Test set\", s=60)\n",
    "axes[0].legend(loc='upper left')\n",
    "axes[0].set_title(\"Original Data\")\n",
    "\n",
    "# scale the data using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# visualize the properly scaled data\n",
    "axes[1].scatter(X_train_scaled[:, 0], X_train_scaled[:, 1], c=mglearn.cm2(0), label=\"Training set\", s=60)\n",
    "axes[1].scatter(X_test_scaled[:, 0], X_test_scaled[:, 1], marker='^', c=mglearn.cm2(1), label=\"Test set\", s=60)\n",
    "axes[1].set_title(\"Scaled Data\")\n",
    "\n",
    "# rescale the test set separately\n",
    "# so test set min is 0 and test set max is 1\n",
    "# DO NOT DO THIS! For illustration purposes only.\n",
    "test_scaler = MinMaxScaler()\n",
    "test_scaler.fit(X_test) #테스트데이터는 별도로 fit하면 안된다는거 알려줌\n",
    "X_test_scaled_badly = test_scaler.transform(X_test)\n",
    "\n",
    "# visualize wrongly scaled data\n",
    "axes[2].scatter(X_train_scaled[:, 0], X_train_scaled[:, 1], c=mglearn.cm2(0), label=\"training set\", s=60)\n",
    "axes[2].scatter(X_test_scaled_badly[:, 0], X_test_scaled_badly[:, 1], marker='^', c=mglearn.cm2(1), label=\"test set\", s=60)\n",
    "axes[2].set_title(\"Improperly Scaled Data\")\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xlabel(\"Feature 0\")\n",
    "    ax.set_ylabel(\"Feature 1\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# calling fit and transform in sequence (using method chaining)\n",
    "X_scaled = scaler.fit(X_train).transform(X_train)#한번에 fit과 transform이 됨\n",
    "# same result, but more efficient computation\n",
    "X_scaled_d = scaler.fit_transform(X_train)#위에거 한번에 쓰는거 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.4 The effect of preprocessing on supervised learning\n",
    "- C: 규제의 강도를 결정하는 매개변수\n",
    "  - 높은 C 값: 규제의 감소 --> 훈련 세트에 최대로 맞춤\n",
    "  - 낮은 C 값: 규제의 증대 --> 계수 백터(w)를 최대로 0에 가깝게 만듦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.63\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=0)\n",
    "\n",
    "svm = SVC(C=100)\n",
    "svm.fit(X_train, y_train)\n",
    "print(\"Test set accuracy: {:.2f}\".format(svm.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MinMaxScaler 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled test set accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "# preprocessing using 0-1 scaling\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "svm = SVC(C=100) # 그냥 SVM 쓰면 위에꺼 있는거 fit하는거니까 새로 만드는게 더 맞는거 같다. \n",
    "# learning an SVM on the scaled training data\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "# scoring on the scaled test set\n",
    "print(\"Scaled test set accuracy: {:.2f}\".format(svm.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- StandardScaler 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preprocessing using zero mean and unit variance scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# learning an SVM on the scaled training data\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "# scoring on the scaled test set\n",
    "print(\"SVM test accuracy: {:.2f}\".format(svm.score(X_test_scaled, y_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
